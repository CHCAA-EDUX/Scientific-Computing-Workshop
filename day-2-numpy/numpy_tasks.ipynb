{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Numpy tasks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task Version number\n",
    "> a) extract numpy version number"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arrays\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task: Speed comparison, numpy is faster\n",
    "\n",
    "> a) compute the sum of squared values from 10-10 000 using a list comprehension. This corresponds to:\n",
    "$$\n",
    "\\sum_{i=10}^{10 000}i^2\n",
    "$$\n",
    "\n",
    "> b) compute the same using numpy\n",
    "\n",
    "> c) perform a speed comparison between the two approaches  *Hint* you can measure time using:\n",
    "```\n",
    "import time\n",
    "start = time.time()\n",
    "# your code\n",
    "time_taken = time.time() - start\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task: Speed comparison, no lists are faster\n",
    "\n",
    "1) create a for loop where you append numbers to a list and a numpy array (let's say 1 000 000). Show that list are faster than numpy arrays in this situation.\n",
    "\n",
    "This is a design decision which makes python cheap to append to (read more [here](https://stackoverflow.com/questions/5932328/internals-of-python-list-access-and-resizing-runtimes)).\n",
    "\n",
    "**Bonus**: You can make numpy noticably faster by preallocating memory. E.g. by creating an array of zeros and assigning the values instead of appending them.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating and combining arrays\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task: Combining arrays\n",
    "\n",
    "stack  two  numpy arrays vertically.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "a = np.arange(10).reshape(2,-1)\n",
    "b = np.repeat(1, 10).reshape(2,-1)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "#> array([[0, 1, 2, 3, 4],\n",
    "#>        [5, 6, 7, 8, 9],\n",
    "#>        [1, 1, 1, 1, 1],\n",
    "#>        [1, 1, 1, 1, 1]])\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Indexing arrays\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task: Extract the middle\n",
    "> extract the middle row in an 1D array. If there is an even amount of arrays extract the two middle rows.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "d = np.array([1, 2, 3])\n",
    "print(get_middle(d))\n",
    "print(get_middle(np.arange(4)))\n",
    "```\n",
    "\n",
    "Output\n",
    "```\n",
    "2\n",
    "[1 2]\n",
    "```\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task: Word frequencies\n",
    "> a) Given one array (of strings) select all values in a corresponding  array that corresponds to the highest values in the initial array.\n",
    "\n",
    "**Example:** Most frequent words\n",
    "```\n",
    "words = np.array([\"a\", \"duck\", \"dog\", \"the\"])\n",
    "counts = np.array([30, 2, 4, 110])\n",
    "print(f(words, counts))\n",
    "```\n",
    "\n",
    "Output\n",
    "```\n",
    "\"the\"\n",
    "```\n",
    "\n",
    "> bonus: generalize it to the n highest values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Formula to Code\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task: The Euclidian Norm\n",
    "> implement the euclidian ($L^2$) norm defined as: \n",
    "\n",
    "$$||x||_2 = \\sqrt{x^2_1 + x^2_2 + ... x^2_n}$$\n",
    "\n",
    "Check that it it gives the same result as `np.linalg.norm`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task: The sigmoid function\n",
    "The sigmoid (or the logistic) function is a function which is normalized between 0 and 1. It is defined as:\n",
    "$$\n",
    "s(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "> Implement the sigmoid function\n",
    "\n",
    "Example:\n",
    "```\n",
    "print(s(0))\n",
    "print(s(2))\n",
    "print(s(100))\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "0.5\n",
    "0.88079...\n",
    "1.0\n",
    "```\n",
    "\n",
    "> **Bonus:** plot the function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task: Vector Distances\n",
    "> Implement the following distances:\n",
    "- Euclidian ($L^2$) distance\n",
    "- Cosine distance\n",
    "- the Hamming distance\n",
    "\n",
    "\n",
    "**The euclidian distance** is a distance measure which denoted the length between two vectors $a$ and $b$ as defined by:\n",
    "$$\n",
    "dist(a, b) = || a-b || \n",
    "$$\n",
    "\n",
    "You can check that it give the same result as `scipy.spatial.distance.euclidean`.\n",
    "\n",
    "**The cosine distance** is a distance measure based on the angle between to vectors, $a$ and $b$. It can be shown that it is equal to: \n",
    "\n",
    "$$\n",
    "dist(a, b) = 1 - cos(\\theta) =  1 - \\frac{a \\cdot b} {|| a || \\cdot || b ||}\n",
    "$$\n",
    "\n",
    "You can check that it give the same result as `scipy.spatial.distance.cosine`.\n",
    "\n",
    "\n",
    "**The Hamming distance** between 1-D arrays $u$ and $v$, is simply the proportion of disagreeing components in $u$ and $v$. \n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "u = [2, 6, 5, 8, 1, 2, 4, 5, 2, 6]\n",
    "v = [0, 0, 5, 8, 1, 2, 4, 5, 2, 6]\n",
    "print(hamming(u, v))\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "0.2\n",
    "```\n",
    "\n",
    "You can check that it give the same result as `scipy.spatial.distance.hamming`.\n",
    "\n",
    "\n",
    "> **Bonus**: Cosine similarity is a similarity measure which is defined as $1 - cos\\_dist$. Why does this constitute a reasonable measure of similarity? *Hint* What is the potential range of values. Try out a few vectors and see.\n",
    "\n",
    "<br /> \n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "  Cosine similarity is normalized. That means that if the vector are the same (ignore length/magnitude) the cosine similarity is 1 (i.e. they as similar as can be). So the closer it is to 1 the more similar the vectors are.\n",
    "</details>\n",
    "\n",
    "<br /> \n",
    "\n",
    "\n",
    "> **Bonus**: Why would you want to use cosine distance instead of the euclidian norm?\n",
    "\n",
    "<br /> \n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "  Cosine distance, or cosine similarity, is typically used to denote the distance between two vectors ignoring magnitude. Actually it can be [shown](https://stats.stackexchange.com/questions/146221/is-cosine-similarity-identical-to-l2-normalized-euclidean-distance) that euclidian norm of a normalized vector is proportional to the cosine distance.\n",
    "</details>\n",
    "\n",
    "<br /> \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task: Shannon Enthropy\n",
    "\n",
    "Calculate the shannon entropy of the follow dice throws:\n",
    "```\n",
    "[1, 2, 3, 4, 5, 6]\n",
    "[1, 2, 1, 3, 1, 4]\n",
    "[6, 5, 5, 6, 5, 4]\n",
    "```\n",
    "Both given a standard dice probability $\\frac{1}{6}$ for each outcome and  weighted dice with the probabilites:\n",
    "\n",
    "`{1: 0.1, 2: 0.1, 3: 0.1, 4: 0.2, 5: 0.2, 6: 0.3}`\n",
    "\n",
    "Shannon entropy is defined as:\n",
    "\n",
    "$$\n",
    "entropy(x) := \\sum_{i=1}^n P(x_i) \\cdot log P(x_i)\n",
    "$$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task Finding the minimum of a parabola\n",
    "\n",
    "> 1) program a function that given a list of grid $x$ and a tuple with three entries ($b_1$, $b_2$, $b_3$) computes the parabola $b_1 + b_2x + b_3x^2$ \n",
    "\n",
    "```python\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "y = f(x, betas=(1, 1.4, 2)) \n",
    "```\n",
    "\n",
    "Which corresponds to $1 + 1.4x + 2x^2$.\n",
    "\n",
    "> 2) Program a function which calculates the gradient $f'(x_i)$ using derivatives.\n",
    "\n",
    "<br /> \n",
    "\n",
    "<details>\n",
    "  <summary>Help with derivatives</summary>\n",
    "\n",
    "---\n",
    "\n",
    "To solve the task it might be convenient to know:\n",
    "- The derivative of a constant is 0 \n",
    "\n",
    "$\\frac{d}{dx} c = 0$\n",
    "\n",
    "- The derivative of $c \\cdot x$ is $c$\n",
    "\n",
    "$\\frac{d}{dx} cx = c$\n",
    "\n",
    "- The derivative of $x^n$ is $n \\cdot x^{n-1}$\n",
    "\n",
    "$\\frac{d}{dx} x^n = n \\cdot x^{n-1}$\n",
    "\n",
    "---\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "For example:\n",
    "```python\n",
    "x = 2\n",
    "gradient = df(x, betas=(1, 1.4, 2))\n",
    "print(gradient)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "9.4\n",
    "```\n",
    "> 3) Using 2) create a function which given an initial starting state i) computes gradient ii) minimizes the gradient by taking a small step (learning rate) toward the minimum iii) keep doing this `n` times\n",
    "\n",
    "Test that you function get similar results as you would get using provided `n` is high enough and a reasonable learning rate:\n",
    "```\n",
    "x[y.argmin()]  # which value of x corresponds to the minimum of y\n",
    "```\n",
    "\n",
    "**Bonus**: Instead of setting an `n` make it so that the functions stops when there only little "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task: Ordinary Least Squares\n",
    "\n",
    "Ordinary least squares (OLS) is a popular apprach for estimating a linear fit. Its matrix formulation is given by:\n",
    "\n",
    "$$\n",
    "(X^TX)\\hat{\\beta}=X^Ty\n",
    "\\Rightarrow\n",
    "\\hat{\\beta}=(X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "> Calculate the OLS of the given datapoints below.\n",
    "\n",
    "Feel free to check your results using `lm()` in R or using [`statsmodels`](https://www.statsmodels.org/stable/regression.html) or [`sklearn`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "**Bonus:** Plot the fit, does it make sense?\n",
    "\n",
    "**Bonus:** Using OLS fit a quadratic polynomial\n",
    "\n",
    "**Bonus:** lm() in python. R has a convenient function `lm` where you fit a function using a string formula and dataset. Create a similar function in python by parsing the string. This would look something like this: \n",
    "```\n",
    "data = {\"x\": np.array([0, 1, 2, 3]), \"y\": np.array([-1, 0.2, 0.9, 2.1]) }\n",
    "lm(formula=\"y~1 + x\", data=data)\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# our x, y datapoints\n",
    "x = np.array([0, 1, 2, 3])\n",
    "y = np.array([-1, 0.2, 0.9, 2.1])\n",
    "\n",
    "# add a list of ones denoting intercept\n",
    "X = np.vstack([np.ones(len(x)), x]).T\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "# solve using OLS:\n",
    "# ..."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task: One-hot encoding\n",
    "\n",
    "One-hot encoding denote transforming a series of categories (e.g. \"Natural language processing\", \"Neuroscience\" ... or 1, 2, ...) into a matrix of shape (*, num_classes) that have zeros \n",
    "everywhere except where the index (of the last dimension) mathces the corresponding value of the input label, in which case it will be 1. For instance the vector `[0, 1, 0]` could denote \"Neuroscience\".\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "np.random.seed(101) \n",
    "arr = np.random.randint(1,4, size=6)\n",
    "print(arr)\n",
    "print(one_hot(arr))\n",
    "\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "array([2, 3, 2, 2, 2, 1])\n",
    "array([[ 0.,  1.,  0.],\n",
    "       [ 0.,  0.,  1.],\n",
    "       [ 0.,  1.,  0.],\n",
    "       [ 0.,  1.,  0.],\n",
    "       [ 0.,  1.,  0.],\n",
    "       [ 1.,  0.,  0.]])\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Additional Task\n",
    "\n",
    "---\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Array"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task: Creating the bullseye\n",
    "\n",
    "> Create a function which for an input shape (Tuple[int, int]) return a matrix of the same shape where the border are ones, the middle is 8 and the remainder is zeros.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "print(bullseye((5, 5)))\n",
    "print(bullseye((5, 6)))\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "[[1. 1. 1. 1. 1.]\n",
    " [1. 0. 0. 0. 1.]\n",
    " [1. 0. 9. 0. 1.]\n",
    " [1. 0. 0. 0. 1.]\n",
    " [1. 1. 1. 1. 1.]]\n",
    "\n",
    "[[1. 1. 1. 1. 1. 1.]\n",
    " [1. 0. 0. 0. 0. 1.]\n",
    " [1. 0. 9. 9. 0. 1.]\n",
    " [1. 0. 0. 0. 0. 1.]\n",
    " [1. 1. 1. 1. 1. 1.]]\n",
    "\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('NLP': venv)"
  },
  "interpreter": {
   "hash": "2136a9c3637fd160483224d7922e48bf03b650be5dff26724a0c1f8d1279953b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}